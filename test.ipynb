{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d80d12e",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be5ee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from DataPrep import preprocess, remove_rare_words\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a273a838",
   "metadata": {},
   "source": [
    "# Support Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db589db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(X_train, n_gram=(1,1)):\n",
    "    vectorizer = CountVectorizer(ngram_range=n_gram)\n",
    "    dt_matrix = vectorizer.fit_transform(X_train).toarray()\n",
    "    return vectorizer, dt_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d99805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(lim, genres, parameter, amount, accuracy, f1score, additional: list[tuple] = []):\n",
    "    '''Save the output of the tests to a csv file'''\n",
    "    # Create the location of the file\n",
    "    location = f'./output/{lim}/{parameter}/'\n",
    "    Path(location).mkdir(parents=True, exist_ok=True)\n",
    "    file = location + f'{\"_\".join(genres)}.csv'\n",
    "\n",
    "    # Prep csv format data\n",
    "    columns = [parameter, 'accuracy', 'f1score']\n",
    "    data = [[amount, accuracy, f1score]]\n",
    "    # Add optional additional parameters (like feature space size)\n",
    "    for param, amt in additional:\n",
    "        columns.append(param)\n",
    "        data[0].append(amt)\n",
    "\n",
    "    # Write to csv\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    header = not os.path.exists(file)\n",
    "    df.to_csv(file, mode='a', header=header)\n",
    "\n",
    "    print(f'Succesfully saved output to file {file}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40bf9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(documents, df, genres, parameter, amount, limit, additional=[]):\n",
    "    '''Run the test and save the output'''\n",
    "    # Prepare data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(documents, df['tag'], test_size=0.1, random_state = 42)\n",
    "    vectorizer, dt_matrix = vectorize(X_train)\n",
    "\n",
    "    # Fit model\n",
    "    if parameter != 'model':\n",
    "        model = MultinomialNB()\n",
    "    else:\n",
    "        model = amount\n",
    "        amount = model.__name__\n",
    "    model.fit(dt_matrix, y_train)\n",
    "\n",
    "    # Test\n",
    "    dt_matrix_test = vectorizer.transform(X_test)\n",
    "    y_pred = model.predict(dt_matrix_test)\n",
    "\n",
    "    # Calculate and save results\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    save_output(limit, genres, parameter, amount, accuracy, f1, additional=additional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f0beb9",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8328700",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa11cc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      3\u001b[39m limit = \u001b[32m1000\u001b[39m\n\u001b[32m      5\u001b[39m df = pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdata/song_lyrics_reduced_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m.join(genres)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m documents = \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlyrics\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m run(documents, df, genres, \u001b[33m'\u001b[39m\u001b[33mNormal\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mNormal\u001b[39m\u001b[33m'\u001b[39m, limit)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\DataPrep.py:207\u001b[39m, in \u001b[36mpreprocess\u001b[39m\u001b[34m(docs, stem_words, limit, debug, return_count, use_rid, line_quants, token_quants, tpl_quants, use_length)\u001b[39m\n\u001b[32m    204\u001b[39m normalized = normalize(just_words)\n\u001b[32m    206\u001b[39m \u001b[38;5;66;03m# Sophisticated feature: RID\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m rid_values = \u001b[43mget_rid_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug \u001b[38;5;129;01mand\u001b[39;00m use_rid > \u001b[32m0\u001b[39m: \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mAssigned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(rid_values)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m RID values (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mset\u001b[39m(rid_values))\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug: \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTokenizing\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\DataPrep.py:53\u001b[39m, in \u001b[36mget_rid_values\u001b[39m\u001b[34m(documents)\u001b[39m\n\u001b[32m     51\u001b[39m results = []\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     result = \u001b[43mRID\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     counts = {category: \u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m CATEGORIES}\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m result.category_count.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\rid.py:129\u001b[39m, in \u001b[36mRegressiveImageryDictionary.analyze\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.token_is_excluded(token):\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m         category = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m category != \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    131\u001b[39m             increment_category(category, token)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\rid.py:110\u001b[39m, in \u001b[36mRegressiveImageryDictionary.get_category\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtoken_is_excluded\u001b[39m(\u001b[38;5;28mself\u001b[39m, token):\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.exclusion_pattern.match(token)\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_category\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[32m    111\u001b[39m     categories = \u001b[38;5;28mself\u001b[39m.pattern_tree.retrieve(word)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m categories:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Regular run\n",
    "genres = ['rap', 'rock', 'rb', 'country']\n",
    "limit = 1000\n",
    "\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "\n",
    "documents = preprocess(df['lyrics'], debug=True)\n",
    "run(documents, df, genres, 'Normal', 'Normal', limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f532fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 400 RID values (['PRIMARY', 'SECONDARY', 'EMOTIONS'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/100/Limit/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'SECONDARY', 'EMOTIONS'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Limit/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 40000 RID values (['PRIMARY', 'None', 'SECONDARY', 'EMOTIONS'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/10000/Limit/rap_rock_rb_country.csv\n"
     ]
    }
   ],
   "source": [
    "# 100 vs 1000 vs 10000\n",
    "genres = ['rap', 'rock', 'rb', 'country']\n",
    "limits = [100, 1000, 10000]\n",
    "\n",
    "for limit in limits:\n",
    "    df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "\n",
    "    documents = preprocess(df['lyrics'], debug=True)\n",
    "    run(documents, df, genres, 'Limit', limit, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e79d7c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - tear-stopp: 1 - 80-proof: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 0 or less times\n",
      "Reduced vocab from 27579 to 27579 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - tear-stopp: 1 - 80-proof: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 1 or less times\n",
      "Reduced vocab from 27579 to 13956 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 468832\n",
      "Unique words: 13956\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "granddaddi: 2 - pick-up: 2 - taillight: 2 - ooh-oh-oh: 2 - footloos: 2 - hardwork: 2 - fifty-fifti: 2 - 319: 2 - smallest: 2 - cutoff: 2\n",
      "\n",
      "Removed all words occuring 2 or less times\n",
      "Reduced vocab from 13956 to 10457 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 461834\n",
      "Unique words: 10457\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "redwood: 3 - neath: 3 - cannonbal: 3 - tupelo: 3 - boondock: 3 - pontiac: 3 - wildfir: 3 - drunken: 3 - bethlehem: 3 - suntan: 3\n",
      "\n",
      "Removed all words occuring 3 or less times\n",
      "Reduced vocab from 10457 to 8624 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 456335\n",
      "Unique words: 8624\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "tumblin: 4 - draggin: 4 - honeymoon: 4 - rufu: 4 - albuquerqu: 4 - songwrit: 4 - wide-ey: 4 - yon: 4 - bocephu: 4 - christmas: 4\n",
      "\n",
      "Removed all words occuring 4 or less times\n",
      "Reduced vocab from 8624 to 7433 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 451571\n",
      "Unique words: 7433\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "outskirt: 5 - lovesick: 5 - good-by: 5 - swv: 5 - mmmmm: 5 - darkchild: 5 - xo: 5 - whoa-oh-oh: 5 - sylvest: 5 - bridget: 5\n",
      "\n",
      "Removed all words occuring 5 or less times\n",
      "Reduced vocab from 7433 to 6598 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 447396\n",
      "Unique words: 6598\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "firefli: 6 - prowl: 6 - strokin: 6 - liber: 6 - in-between: 6 - incomplet: 6 - ciara: 6 - yeahhhh: 6 - honeysuckl: 6 - gwendolyn: 6\n",
      "\n",
      "Removed all words occuring 6 or less times\n",
      "Reduced vocab from 6598 to 5991 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 443754\n",
      "Unique words: 5991\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "bittersweet: 7 - smoki: 7 - postcard: 7 - ooohh: 7 - septemb: 7 - yeahh: 7 - t-boz: 7 - rockland: 7 - gill: 7 - policeman: 7\n",
      "\n",
      "Removed all words occuring 7 or less times\n",
      "Reduced vocab from 5991 to 5464 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 440065\n",
      "Unique words: 5464\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "bind: 8 - pillar: 8 - tin: 8 - ember: 8 - solut: 8 - nashvil: 8 - barefoot: 8 - oooooh: 8 - homemad: 8 - romeo: 8\n",
      "\n",
      "Removed all words occuring 8 or less times\n",
      "Reduced vocab from 5464 to 5070 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 436913\n",
      "Unique words: 5070\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "harmon: 9 - paranoia: 9 - sneakin: 9 - whirl: 9 - wha: 9 - mend: 9 - tequila: 9 - ey: 9 - feenin: 9 - honky-tonk: 9\n",
      "\n",
      "Removed all words occuring 9 or less times\n",
      "Reduced vocab from 5070 to 4713 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 433700\n",
      "Unique words: 4713\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "haili: 10 - mon: 10 - brass: 10 - woah-oh: 10 - sensat: 10 - cattl: 10 - decay: 10 - thi: 10 - oh-oh-oh-oh: 10 - woah-woah: 10\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 4713 to 4411 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 430680\n",
      "Unique words: 4411\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "ish: 11 - mortal: 11 - undo: 11 - sooth: 11 - hillbilli: 11 - headin: 11 - hallelujah: 11 - spiral: 11 - stormi: 11 - gravel: 11\n",
      "\n",
      "Removed all words occuring 11 or less times\n",
      "Reduced vocab from 4411 to 4146 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 427765\n",
      "Unique words: 4146\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "witcha: 12 - puppi: 12 - yearn: 12 - nuff: 12 - teardrop: 12 - hose: 12 - sparkl: 12 - loft: 12 - crave: 12 - ooo: 12\n",
      "\n",
      "Removed all words occuring 12 or less times\n",
      "Reduced vocab from 4146 to 3910 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 424933\n",
      "Unique words: 3910\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "straw: 13 - lend: 13 - jah: 13 - believin: 13 - unwind: 13 - keg: 13 - joker: 13 - easier: 13 - rodeo: 13 - tonk: 13\n",
      "\n",
      "Removed all words occuring 13 or less times\n",
      "Reduced vocab from 3910 to 3722 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 422489\n",
      "Unique words: 3722\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "firework: 14 - scatter: 14 - jon: 14 - mystic: 14 - moonshin: 14 - tlc: 14 - mamma: 14 - rusti: 14 - compromis: 14 - honki: 14\n",
      "\n",
      "Removed all words occuring 14 or less times\n",
      "Reduced vocab from 3722 to 3549 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 420067\n",
      "Unique words: 3549\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "unknown: 15 - farmer: 15 - gown: 15 - terribl: 15 - obsess: 15 - shallow: 15 - trucker: 15 - romant: 15 - seam: 15 - thru: 15\n",
      "\n",
      "Removed all words occuring 15 or less times\n",
      "Reduced vocab from 3549 to 3397 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 417787\n",
      "Unique words: 3397\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "courag: 16 - rambl: 16 - pawn: 16 - floatin: 16 - yall: 16 - hoo: 16 - void: 16 - aisl: 16 - loneli: 16 - tailgat: 16\n",
      "\n",
      "Removed all words occuring 16 or less times\n",
      "Reduced vocab from 3397 to 3280 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 415915\n",
      "Unique words: 3280\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "retir: 17 - awhil: 17 - safeti: 17 - bliss: 17 - discov: 17 - remedi: 17 - ordinari: 17 - hay: 17 - towel: 17 - trembl: 17\n",
      "\n",
      "Removed all words occuring 17 or less times\n",
      "Reduced vocab from 3280 to 3163 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 413926\n",
      "Unique words: 3163\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "roar: 18 - kitti: 18 - spotlight: 18 - perfum: 18 - cheatin: 18 - undon: 18 - mate: 18 - sailor: 18 - photograph: 18 - factori: 18\n",
      "\n",
      "Removed all words occuring 18 or less times\n",
      "Reduced vocab from 3163 to 3050 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 411892\n",
      "Unique words: 3050\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "tunnel: 19 - inner: 19 - silk: 19 - linger: 19 - outlaw: 19 - toll: 19 - sidewalk: 19 - circu: 19 - jukebox: 19 - tractor: 19\n",
      "\n",
      "Removed all words occuring 19 or less times\n",
      "Reduced vocab from 3050 to 2934 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 409688\n",
      "Unique words: 2934\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "slum: 20 - gym: 20 - drill: 20 - poss: 20 - atmospher: 20 - naughti: 20 - bloom: 20 - drawn: 20 - dim: 20 - lonesom: 20\n",
      "\n",
      "Removed all words occuring 20 or less times\n",
      "Reduced vocab from 2934 to 2828 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 407568\n",
      "Unique words: 2828\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "petti: 21 - denim: 21 - lipstick: 21 - darkest: 21 - indian: 21 - sore: 21 - dive: 21 - intro: 21 - neon: 21 - wander: 21\n",
      "\n",
      "Removed all words occuring 21 or less times\n",
      "Reduced vocab from 2828 to 2746 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 405846\n",
      "Unique words: 2746\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "kissin: 22 - dove: 22 - program: 22 - jazz: 22 - z: 22 - solid: 22 - greater: 22 - dealt: 22 - headlight: 22 - gentl: 22\n",
      "\n",
      "Removed all words occuring 22 or less times\n",
      "Reduced vocab from 2746 to 2656 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 403866\n",
      "Unique words: 2656\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "dancer: 23 - pine: 23 - london: 23 - giant: 23 - heater: 23 - verbal: 23 - princess: 23 - ford: 23 - divorc: 23 - dusti: 23\n",
      "\n",
      "Removed all words occuring 23 or less times\n",
      "Reduced vocab from 2656 to 2586 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 402256\n",
      "Unique words: 2586\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "memphi: 24 - selfish: 24 - award: 24 - metaphor: 24 - creek: 24 - kingdom: 24 - ador: 24 - blanket: 24 - crank: 24 - darlin: 24\n",
      "\n",
      "Removed all words occuring 24 or less times\n",
      "Reduced vocab from 2586 to 2512 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 400480\n",
      "Unique words: 2512\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "swift: 25 - bewar: 25 - china: 25 - marshal: 25 - meanwhil: 25 - cow: 25 - passeng: 25 - x2: 25 - what: 25 - tender: 25\n",
      "\n",
      "Removed all words occuring 25 or less times\n",
      "Reduced vocab from 2512 to 2452 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 398980\n",
      "Unique words: 2452\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "phat: 26 - jock: 26 - unit: 26 - confid: 26 - cha: 26 - balanc: 26 - forgotten: 26 - whack: 26 - maker: 26 - cotton: 26\n",
      "\n",
      "Removed all words occuring 26 or less times\n",
      "Reduced vocab from 2452 to 2391 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 397394\n",
      "Unique words: 2391\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "whistl: 27 - imposs: 27 - fag: 27 - breathin: 27 - david: 27 - intern: 27 - sweatin: 27 - footbal: 27 - seventeen: 27 - suddenli: 27\n",
      "\n",
      "Removed all words occuring 27 or less times\n",
      "Reduced vocab from 2391 to 2332 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 395801\n",
      "Unique words: 2332\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "sudden: 28 - flowin: 28 - condom: 28 - bold: 28 - fightin: 28 - grain: 28 - summertim: 28 - hum: 28 - wed: 28 - paradis: 28\n",
      "\n",
      "Removed all words occuring 28 or less times\n",
      "Reduced vocab from 2332 to 2288 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 394569\n",
      "Unique words: 2288\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "afternoon: 29 - clever: 29 - ach: 29 - fought: 29 - tore: 29 - t-shirt: 29 - will: 29 - mm: 29 - embrac: 29 - ooh-ooh: 29\n",
      "\n",
      "Removed all words occuring 29 or less times\n",
      "Reduced vocab from 2288 to 2241 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 393206\n",
      "Unique words: 2241\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "chief: 30 - tennesse: 30 - habit: 30 - dem: 30 - telephon: 30 - paus: 30 - inde: 30 - familiar: 30 - dreamin: 30 - complic: 30\n",
      "\n",
      "Removed all words occuring 30 or less times\n",
      "Reduced vocab from 2241 to 2193 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 391766\n",
      "Unique words: 2193\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "monday: 31 - singer: 31 - heh: 31 - riot: 31 - ha-ha: 31 - french: 31 - glove: 31 - equal: 31 - bloodi: 31 - fog: 31\n",
      "\n",
      "Removed all words occuring 31 or less times\n",
      "Reduced vocab from 2193 to 2136 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 389999\n",
      "Unique words: 2136\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "sneak: 32 - bean: 32 - lion: 32 - servic: 32 - thrown: 32 - breakin: 32 - curtain: 32 - sacrific: 32 - alcohol: 32 - endless: 32\n",
      "\n",
      "Removed all words occuring 32 or less times\n",
      "Reduced vocab from 2136 to 2093 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 388623\n",
      "Unique words: 2093\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "shatter: 33 - emerg: 33 - corn: 33 - echo: 33 - appeal: 33 - appear: 33 - aye: 33 - dial: 33 - electr: 33 - im: 33\n",
      "\n",
      "Removed all words occuring 33 or less times\n",
      "Reduced vocab from 2093 to 2051 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 387237\n",
      "Unique words: 2051\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "stumbl: 34 - defeat: 34 - scoop: 34 - boogi: 34 - solo: 34 - sentenc: 34 - haunt: 34 - written: 34 - preacher: 34 - brave: 34\n",
      "\n",
      "Removed all words occuring 34 or less times\n",
      "Reduced vocab from 2051 to 2021 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 386217\n",
      "Unique words: 2021\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "scari: 35 - kush: 35 - virgin: 35 - paul: 35 - begun: 35 - especi: 35 - onto: 35 - distanc: 35 - penni: 35 - panti: 35\n",
      "\n",
      "Removed all words occuring 35 or less times\n",
      "Reduced vocab from 2021 to 1976 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 384642\n",
      "Unique words: 1976\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "breakfast: 36 - cherri: 36 - fashion: 36 - replac: 36 - damag: 36 - sword: 36 - bowl: 36 - bitter: 36 - stair: 36 - tide: 36\n",
      "\n",
      "Removed all words occuring 36 or less times\n",
      "Reduced vocab from 1976 to 1937 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 383238\n",
      "Unique words: 1937\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "actual: 37 - abl: 37 - phantom: 37 - steppin: 37 - march: 37 - possibl: 37 - whoo: 37 - glori: 37 - lightn: 37 - rag: 37\n",
      "\n",
      "Removed all words occuring 37 or less times\n",
      "Reduced vocab from 1937 to 1893 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 381610\n",
      "Unique words: 1893\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "femal: 38 - basement: 38 - mommi: 38 - rid: 38 - suffer: 38 - preciou: 38 - appreci: 38 - mighti: 38 - captain: 38 - relationship: 38\n",
      "\n",
      "Removed all words occuring 38 or less times\n",
      "Reduced vocab from 1893 to 1849 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 379938\n",
      "Unique words: 1849\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "resist: 39 - agre: 39 - mysteri: 39 - purs: 39 - johnni: 39 - fals: 39 - sixteen: 39 - miracl: 39 - concern: 39 - ash: 39\n",
      "\n",
      "Removed all words occuring 39 or less times\n",
      "Reduced vocab from 1849 to 1809 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 378378\n",
      "Unique words: 1809\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "bumpin: 40 - dogg: 40 - origin: 40 - cheek: 40 - industri: 40 - tea: 40 - mass: 40 - disguis: 40 - refus: 40 - cast: 40\n",
      "\n",
      "Removed all words occuring 40 or less times\n",
      "Reduced vocab from 1809 to 1770 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 376818\n",
      "Unique words: 1770\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "slam: 41 - gay: 41 - click: 41 - group: 41 - abus: 41 - toy: 41 - forward: 41 - total: 41 - oh-oh-oh: 41 - farm: 41\n",
      "\n",
      "Removed all words occuring 41 or less times\n",
      "Reduced vocab from 1770 to 1729 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 375137\n",
      "Unique words: 1729\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "skirt: 42 - dyin: 42 - faggot: 42 - turnin: 42 - gain: 42 - etern: 42 - truli: 42 - lem: 42 - spine: 42 - shakin: 42\n",
      "\n",
      "Removed all words occuring 42 or less times\n",
      "Reduced vocab from 1729 to 1706 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 374171\n",
      "Unique words: 1706\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "celebr: 43 - lyin: 43 - physic: 43 - self: 43 - dat: 43 - offic: 43 - o'clock: 43 - jackson: 43 - stole: 43 - cage: 43\n",
      "\n",
      "Removed all words occuring 43 or less times\n",
      "Reduced vocab from 1706 to 1674 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 372795\n",
      "Unique words: 1674\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "relat: 44 - stronger: 44 - missin: 44 - hollywood: 44 - silver: 44 - droppin: 44 - toward: 44 - cowboy: 44 - blade: 44 - energi: 44\n",
      "\n",
      "Removed all words occuring 44 or less times\n",
      "Reduced vocab from 1674 to 1651 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 371783\n",
      "Unique words: 1651\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "charg: 45 - fortun: 45 - awak: 45 - charm: 45 - burnin: 45 - hov: 45 - ruin: 45 - apolog: 45 - thunder: 45 - downtown: 45\n",
      "\n",
      "Removed all words occuring 45 or less times\n",
      "Reduced vocab from 1651 to 1627 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 370703\n",
      "Unique words: 1627\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "credit: 46 - greatest: 46 - ow: 46 - honest: 46 - suicid: 46 - heel: 46 - inch: 46 - coffe: 46 - tail: 46 - spell: 46\n",
      "\n",
      "Removed all words occuring 46 or less times\n",
      "Reduced vocab from 1627 to 1597 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 369323\n",
      "Unique words: 1597\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "beast: 47 - fo: 47 - hall: 47 - lifetim: 47 - nervou: 47 - compani: 47 - releas: 47 - sail: 47 - allow: 47 - rebel: 47\n",
      "\n",
      "Removed all words occuring 47 or less times\n",
      "Reduced vocab from 1597 to 1554 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 367302\n",
      "Unique words: 1554\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "floss: 48 - bathroom: 48 - bedroom: 48 - metal: 48 - dedic: 48 - drove: 48 - parent: 48 - stomp: 48 - secur: 48 - thin: 48\n",
      "\n",
      "Removed all words occuring 48 or less times\n",
      "Reduced vocab from 1554 to 1529 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n",
      "Total words: 366102\n",
      "Unique words: 1529\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "destini: 49 - vibe: 49 - famou: 49 - wipe: 49 - slave: 49 - faster: 49 - nail: 49 - boyfriend: 49 - ceil: 49 - ohhh: 49\n",
      "\n",
      "Removed all words occuring 49 or less times\n",
      "Reduced vocab from 1529 to 1507 words\n",
      "Succesfully saved output to file ./output/1000/Cutoff/rap_rock_rb_country.csv\n"
     ]
    }
   ],
   "source": [
    "# Cutoff rate\n",
    "genres = ['rap', 'rock', 'rb', 'country']\n",
    "limit = 1000\n",
    "\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "old, new, documents = preprocess(df['lyrics'], limit=0, debug=True, return_count=True)\n",
    "joined_docs = [' '.join(document) for document in documents]\n",
    "run(joined_docs, df, genres, 'Cutoff', 0, limit, additional=[('features', new)])\n",
    "\n",
    "for cutoff in range(1, 50):\n",
    "    old, new, documents = remove_rare_words(documents, limit=cutoff, debug=True, return_count=True)\n",
    "    joined_docs = [' '.join(document) for document in documents]\n",
    "    run(joined_docs, df, genres, 'Cutoff', cutoff, limit, additional=[('features', new)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf917fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Stem/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Lemmatizing\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Stem/rap_rock_rb_country.csv\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize vs. Stem\n",
    "\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "\n",
    "for stem in [True, False]:\n",
    "    documents = preprocess(df['lyrics'], debug=True, stem_words=stem)\n",
    "    run(documents, df, genres, 'Stem', stem, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "66f13b9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - tear-stopp: 1 - 80-proof: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n"
     ]
    }
   ],
   "source": [
    "# Updated settings\n",
    "genres = ['rap', 'rock', 'rb', 'country']\n",
    "limit = 1000\n",
    "\n",
    "cutoff = 10\n",
    "stem = True\n",
    "\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "documents = preprocess(df['lyrics'], debug=True, limit=cutoff, stem_words=stem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78356335",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully saved output to file ./output/1000/Model/rap_rock_rb_country.csv\n",
      "Succesfully saved output to file ./output/1000/Model/rap_rock_rb_country.csv\n"
     ]
    }
   ],
   "source": [
    "# MultinomialNB vs Logistic Regression\n",
    "models = [MultinomialNB(), LogisticRegression(random_state=42, max_iter=250)]\n",
    "names = ['MultinomialNB', 'LogisticRegression']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, df['tag'], test_size=0.1, random_state = 42)\n",
    "vectorizer, dt_matrix = vectorize(X_train)\n",
    "\n",
    "for i in range(len(models)):\n",
    "    model = models[i]\n",
    "    name = names[i]\n",
    "    model.fit(dt_matrix, y_train)\n",
    "\n",
    "    dt_matrix_test = vectorizer.transform(X_test)\n",
    "    y_pred = model.predict(dt_matrix_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    save_output(limit, genres, 'Model', name, accuracy, f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "843ddae4",
   "metadata": {},
   "source": [
    "### Other Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae45a689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'SECONDARY', 'EMOTIONS'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/RID/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'SECONDARY', 'EMOTIONS'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - tear-stopp: 1 - 80-proof: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/RID/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'SECONDARY', 'EMOTIONS'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - tear-stopp: 1 - 80-proof: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 20 or less times\n",
      "Reduced vocab from 27579 to 2828 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/RID/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'SECONDARY', 'EMOTIONS'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - tear-stopp: 1 - 80-proof: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 30 or less times\n",
      "Reduced vocab from 27579 to 2193 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/RID/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'SECONDARY', 'EMOTIONS'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - tear-stopp: 1 - 80-proof: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 40 or less times\n",
      "Reduced vocab from 27579 to 1770 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/RID/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'SECONDARY', 'EMOTIONS'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - tear-stopp: 1 - 80-proof: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 50 or less times\n",
      "Reduced vocab from 27579 to 1490 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/RID/rap_rock_rb_country.csv\n"
     ]
    }
   ],
   "source": [
    "# RID\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "for cutoff in [0, 10, 20, 30, 40, 50]:\n",
    "    for rid in range(20, 21):\n",
    "        documents = preprocess(df['lyrics'], debug=True, limit=cutoff, stem_words=stem, use_rid=rid)\n",
    "        run(documents, df, genres, 'RID', rid, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ea51a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length\n",
    "genres = ['rap', 'rock', 'rb', 'country']\n",
    "limit = 1000\n",
    "cutoff = 10\n",
    "stem = True\n",
    "rid=20\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "91331635",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 64. 389.] for [0.5, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 52.  79. 389.] for [0.3333333333333333, 0.6666666666666666, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 46.  64.  90. 389.] for [0.25, 0.5, 0.75, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 42.  56.  73.  95. 389.] for [0.2, 0.4, 0.6, 0.8, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 39.   52.   64.   79.   98.5 389. ] for [0.16666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666, 0.8333333333333334, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 37.  48.  59.  71.  86. 102. 389.] for [0.14285714285714285, 0.2857142857142857, 0.42857142857142855, 0.5714285714285714, 0.7142857142857143, 0.8571428571428571, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 35.  46.  54.  64.  76.  90. 105. 389.] for [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 34.  44.  52.  59.  69.  79.  92. 108. 389.] for [0.1111111111111111, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.6666666666666666, 0.7777777777777778, 0.8888888888888888, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 31.          40.          47.          53.18181818  60.\n",
      "  68.          77.          87.          97.         112.\n",
      " 389.        ] for [0.09090909090909091, 0.18181818181818182, 0.2727272727272727, 0.36363636363636365, 0.45454545454545453, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.8181818181818182, 0.9090909090909091, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 30.   39.   46.   52.   58.   64.   72.   79.   90.   98.5 114.  389. ] for [0.08333333333333333, 0.16666666666666666, 0.25, 0.3333333333333333, 0.4166666666666667, 0.5, 0.5833333333333334, 0.6666666666666666, 0.75, 0.8333333333333334, 0.9166666666666666, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 29.  38.  45.  50.  55.  61.  68.  75.  83.  92. 100. 116. 389.] for [0.07692307692307693, 0.15384615384615385, 0.23076923076923078, 0.3076923076923077, 0.38461538461538464, 0.46153846153846156, 0.5384615384615384, 0.6153846153846154, 0.6923076923076923, 0.7692307692307693, 0.8461538461538461, 0.9230769230769231, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 29.  37.  43.  48.  53.  59.  64.  71.  77.  86.  93. 102. 118. 389.] for [0.07142857142857142, 0.14285714285714285, 0.21428571428571427, 0.2857142857142857, 0.35714285714285715, 0.42857142857142855, 0.5, 0.5714285714285714, 0.6428571428571429, 0.7142857142857143, 0.7857142857142857, 0.8571428571428571, 0.9285714285714286, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 28.  36.  42.  47.  52.  56.  61.  67.  73.  79.  88.  95. 103. 119.\n",
      " 389.] for [0.06666666666666667, 0.13333333333333333, 0.2, 0.26666666666666666, 0.3333333333333333, 0.4, 0.4666666666666667, 0.5333333333333333, 0.6, 0.6666666666666666, 0.7333333333333333, 0.8, 0.8666666666666667, 0.9333333333333333, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 28.  35.  41.  46.  50.  54.  59.  64.  70.  76.  82.  90.  96. 105.\n",
      " 121. 389.] for [0.0625, 0.125, 0.1875, 0.25, 0.3125, 0.375, 0.4375, 0.5, 0.5625, 0.625, 0.6875, 0.75, 0.8125, 0.875, 0.9375, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 27.          34.          40.          45.          49.\n",
      "  53.          57.          62.          67.          72.\n",
      "  78.          84.          91.          98.         106.52941176\n",
      " 122.         389.        ] for [0.058823529411764705, 0.11764705882352941, 0.17647058823529413, 0.23529411764705882, 0.29411764705882354, 0.35294117647058826, 0.4117647058823529, 0.47058823529411764, 0.5294117647058824, 0.5882352941176471, 0.6470588235294118, 0.7058823529411765, 0.7647058823529411, 0.8235294117647058, 0.8823529411764706, 0.9411764705882353, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 27.          34.          39.          44.          48.\n",
      "  52.          55.          59.          64.          69.\n",
      "  74.          79.          86.16666667  92.          98.5\n",
      " 108.         123.         389.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 26.          33.          38.          43.          47.\n",
      "  50.          54.          58.          62.          67.\n",
      "  71.          76.          81.15789474  88.          94.\n",
      " 100.         109.         124.         389.        ] for [0.05263157894736842, 0.10526315789473684, 0.15789473684210525, 0.21052631578947367, 0.2631578947368421, 0.3157894736842105, 0.3684210526315789, 0.42105263157894735, 0.47368421052631576, 0.5263157894736842, 0.5789473684210527, 0.631578947368421, 0.6842105263157895, 0.7368421052631579, 0.7894736842105263, 0.8421052631578947, 0.8947368421052632, 0.9473684210526315, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 26.  32.  38.  42.  46.  49.  53.  56.  60.  64.  69.  73.  78.  84.\n",
      "  90.  95. 101. 110. 125. 389.] for [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n"
     ]
    }
   ],
   "source": [
    "for quants in range(2, 21):\n",
    "    documents = preprocess(df['lyrics'], debug=True, limit=cutoff, stem_words=stem, use_rid=rid, line_quants=quants, use_length=rid)\n",
    "    run(documents, df, genres, 'Line counts', quants, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b9a3ab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 391.5 3269. ] for [0.5, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 293.  543. 3269.] for [0.3333333333333333, 0.6666666666666666, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 251.    391.5   622.25 3269.  ] for [0.25, 0.5, 0.75, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 222.  327.  481.  680. 3269.] for [0.2, 0.4, 0.6, 0.8, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 207.5  293.   391.5  543.   724.  3269. ] for [0.16666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666, 0.8333333333333334, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 198.  270.  344.  456.  585.  755. 3269.] for [0.14285714285714285, 0.2857142857142857, 0.42857142857142855, 0.5714285714285714, 0.7142857142857143, 0.8571428571428571, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 187.    251.    316.    391.5   502.    622.25  783.   3269.  ] for [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 179.          235.          293.          351.          443.\n",
      "  543.          653.          805.66666667 3269.        ] for [0.1111111111111111, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.6666666666666666, 0.7777777777777778, 0.8888888888888888, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 172.   222.   278.   327.   391.5  481.   572.   680.   826.  3269. ] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 167.          215.          262.          308.          358.\n",
      "  433.          515.81818182  598.          700.          841.45454545\n",
      " 3269.        ] for [0.09090909090909091, 0.18181818181818182, 0.2727272727272727, 0.36363636363636365, 0.45454545454545453, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.8181818181818182, 0.9090909090909091, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 163.    207.5   251.    293.    338.    391.5   467.    543.    622.25\n",
      "  724.    854.   3269.  ] for [0.08333333333333333, 0.16666666666666666, 0.25, 0.3333333333333333, 0.4166666666666667, 0.5, 0.5833333333333334, 0.6666666666666666, 0.75, 0.8333333333333334, 0.9166666666666666, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 157.          202.23076923  240.          281.          319.\n",
      "  362.          425.          494.          564.53846154  646.\n",
      "  738.76923077  871.         3269.        ] for [0.07692307692307693, 0.15384615384615385, 0.23076923076923078, 0.3076923076923077, 0.38461538461538464, 0.46153846153846156, 0.5384615384615384, 0.6153846153846154, 0.6923076923076923, 0.7692307692307693, 0.8461538461538461, 0.9230769230769231, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 153.   198.   230.   270.   304.   344.   391.5  456.   521.   585.\n",
      "  665.   755.   881.  3269. ] for [0.07142857142857142, 0.14285714285714285, 0.21428571428571427, 0.2857142857142857, 0.35714285714285715, 0.42857142857142855, 0.5, 0.5714285714285714, 0.6428571428571429, 0.7142857142857143, 0.7857142857142857, 0.8571428571428571, 0.9285714285714286, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 149.   191.2  222.   259.   293.   327.   366.2  420.   481.   543.\n",
      "  604.   680.   768.   890.  3269. ] for [0.06666666666666667, 0.13333333333333333, 0.2, 0.26666666666666666, 0.3333333333333333, 0.4, 0.4666666666666667, 0.5333333333333333, 0.6, 0.6666666666666666, 0.7333333333333333, 0.8, 0.8666666666666667, 0.9333333333333333, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 146.      187.      217.8125  251.      283.6875  316.      347.\n",
      "  391.5     448.      502.      560.      622.25    694.      783.\n",
      "  901.     3269.    ] for [0.0625, 0.125, 0.1875, 0.25, 0.3125, 0.375, 0.4375, 0.5, 0.5625, 0.625, 0.6875, 0.75, 0.8125, 0.875, 0.9375, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 144.          183.          213.          242.          275.\n",
      "  302.          335.          369.          417.          471.\n",
      "  525.          578.          640.          709.29411765  793.52941176\n",
      "  905.         3269.        ] for [0.058823529411764705, 0.11764705882352941, 0.17647058823529413, 0.23529411764705882, 0.29411764705882354, 0.35294117647058826, 0.4117647058823529, 0.47058823529411764, 0.5294117647058824, 0.5882352941176471, 0.6470588235294118, 0.7058823529411765, 0.7647058823529411, 0.8235294117647058, 0.8823529411764706, 0.9411764705882353, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 138.47368421  175.          204.          228.          257.\n",
      "  285.          312.          340.          372.          414.\n",
      "  463.21052632  508.          558.          608.          668.\n",
      "  733.57894737  814.05263158  926.         3269.        ] for [0.05263157894736842, 0.10526315789473684, 0.15789473684210525, 0.21052631578947367, 0.2631578947368421, 0.3157894736842105, 0.3684210526315789, 0.42105263157894735, 0.47368421052631576, 0.5263157894736842, 0.5789473684210527, 0.631578947368421, 0.6842105263157895, 0.7368421052631579, 0.7894736842105263, 0.8421052631578947, 0.8947368421052632, 0.9473684210526315, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 135.    172.    201.    222.    251.    278.    301.    327.    354.\n",
      "  391.5   438.    481.    528.    572.    622.25  680.    744.    826.\n",
      "  933.   3269.  ] for [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0] (range 29 - 3269)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n"
     ]
    }
   ],
   "source": [
    "line_quants = 10\n",
    "\n",
    "for quants in range(2, 21):\n",
    "    documents = preprocess(df['lyrics'], debug=True, limit=cutoff, stem_words=stem, use_rid=rid, \n",
    "                           line_quants=line_quants, token_quants=quants, use_length=rid)\n",
    "    run(documents, df, genres, 'Line counts', quants, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb631db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 5.  7. 19.] for [0.3333333333333333, 0.6666666666666666, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 5.  6.  7. 19.] for [0.25, 0.5, 0.75, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 5.  5.  6.  7. 19.] for [0.2, 0.4, 0.6, 0.8, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 4.  5.  6.  7.  8. 19.] for [0.16666666666666666, 0.3333333333333333, 0.5, 0.6666666666666666, 0.8333333333333334, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 4.  5.  6.  6.  7.  8. 19.] for [0.14285714285714285, 0.2857142857142857, 0.42857142857142855, 0.5714285714285714, 0.7142857142857143, 0.8571428571428571, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 4.  5.  5.  6.  6.  7.  8. 19.] for [0.125, 0.25, 0.375, 0.5, 0.625, 0.75, 0.875, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 4.  5.  5.  6.  6.  7.  7.  8. 19.] for [0.1111111111111111, 0.2222222222222222, 0.3333333333333333, 0.4444444444444444, 0.5555555555555556, 0.6666666666666666, 0.7777777777777778, 0.8888888888888888, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 4.  5.  5.  5.  6.  6.  7.  7.  8. 19.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 4.  4.  5.  5.  6.  6.  6.  7.  7.  8. 19.] for [0.09090909090909091, 0.18181818181818182, 0.2727272727272727, 0.36363636363636365, 0.45454545454545453, 0.5454545454545454, 0.6363636363636364, 0.7272727272727273, 0.8181818181818182, 0.9090909090909091, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 4.  4.  5.  5.  6.  6.  6.  7.  7.  8.  8. 19.] for [0.08333333333333333, 0.16666666666666666, 0.25, 0.3333333333333333, 0.4166666666666667, 0.5, 0.5833333333333334, 0.6666666666666666, 0.75, 0.8333333333333334, 0.9166666666666666, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 4.  4.  5.  5.  5.  6.  6.  6.  7.  7.  8.  8. 19.] for [0.07692307692307693, 0.15384615384615385, 0.23076923076923078, 0.3076923076923077, 0.38461538461538464, 0.46153846153846156, 0.5384615384615384, 0.6153846153846154, 0.6923076923076923, 0.7692307692307693, 0.8461538461538461, 0.9230769230769231, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 4.  4.  5.  5.  5.  6.  6.  6.  6.  7.  7.  8.  8. 19.] for [0.07142857142857142, 0.14285714285714285, 0.21428571428571427, 0.2857142857142857, 0.35714285714285715, 0.42857142857142855, 0.5, 0.5714285714285714, 0.6428571428571429, 0.7142857142857143, 0.7857142857142857, 0.8571428571428571, 0.9285714285714286, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 4.  4.  5.  5.  5.  5.  6.  6.  6.  7.  7.  7.  8.  8. 19.] for [0.06666666666666667, 0.13333333333333333, 0.2, 0.26666666666666666, 0.3333333333333333, 0.4, 0.4666666666666667, 0.5333333333333333, 0.6, 0.6666666666666666, 0.7333333333333333, 0.8, 0.8666666666666667, 0.9333333333333333, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 4.  4.  4.  5.  5.  5.  6.  6.  6.  6.  7.  7.  7.  8.  8. 19.] for [0.0625, 0.125, 0.1875, 0.25, 0.3125, 0.375, 0.4375, 0.5, 0.5625, 0.625, 0.6875, 0.75, 0.8125, 0.875, 0.9375, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 4.  4.  4.  5.  5.  5.  5.  6.  6.  6.  7.  7.  7.  7.  8.  8. 19.] for [0.058823529411764705, 0.11764705882352941, 0.17647058823529413, 0.23529411764705882, 0.29411764705882354, 0.35294117647058826, 0.4117647058823529, 0.47058823529411764, 0.5294117647058824, 0.5882352941176471, 0.6470588235294118, 0.7058823529411765, 0.7647058823529411, 0.8235294117647058, 0.8823529411764706, 0.9411764705882353, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 4.  4.  4.  5.  5.  5.  5.  6.  6.  6.  6.  7.  7.  7.  8.  8.  9. 19.] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 3.47368421  4.          4.          5.          5.          5.\n",
      "  5.          6.          6.          6.          6.          6.\n",
      "  7.          7.          7.          8.          8.          9.\n",
      " 19.        ] for [0.05263157894736842, 0.10526315789473684, 0.15789473684210525, 0.21052631578947367, 0.2631578947368421, 0.3157894736842105, 0.3684210526315789, 0.42105263157894735, 0.47368421052631576, 0.5263157894736842, 0.5789473684210527, 0.631578947368421, 0.6842105263157895, 0.7368421052631579, 0.7894736842105263, 0.8421052631578947, 0.8947368421052632, 0.9473684210526315, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 3.  4.  4.  5.  5.  5.  5.  5.  6.  6.  6.  6.  7.  7.  7.  7.  8.  8.\n",
      "  9. 19.] for [0.05, 0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.45, 0.5, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n"
     ]
    }
   ],
   "source": [
    "token_quants = 18\n",
    "\n",
    "for quants in range(2, 21):\n",
    "    documents = preprocess(df['lyrics'], debug=True, limit=cutoff, stem_words=stem, use_rid=rid, \n",
    "                           line_quants=line_quants, token_quants=token_quants, tpl_quants=quants, use_length=rid)\n",
    "    run(documents, df, genres, 'Line counts', quants, limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e94c8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n",
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Assigned 4000 RID values (['PRIMARY', 'EMOTIONS', 'SECONDARY'])\n",
      "Tokenizing\n",
      "Counting tokens\n",
      "Found quantiles: [ 141.16666667  179.          207.5         235.          265.\n",
      "  293.          321.          351.          391.5         443.\n",
      "  490.          543.          593.          653.          724.\n",
      "  805.66666667  912.83333333 3269.        ] for [0.05555555555555555, 0.1111111111111111, 0.16666666666666666, 0.2222222222222222, 0.2777777777777778, 0.3333333333333333, 0.3888888888888889, 0.4444444444444444, 0.5, 0.5555555555555556, 0.6111111111111112, 0.6666666666666666, 0.7222222222222222, 0.7777777777777778, 0.8333333333333334, 0.8888888888888888, 0.9444444444444444, 1.0] (range 29 - 3269)\n",
      "Counting tokens per line\n",
      "Found quantiles: [ 6. 19.] for [0.5, 1.0] (range 2 - 19)\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 482455\n",
      "Unique words: 27579\n",
      "Most frequent:\n",
      "'s: 3468; n't: 3309; 'm: 2776; like: 2750; know: 2675; got: 2552; get: 2447; ': 2201; go: 2165; see: 2020\n",
      "Least frequent:\n",
      "marina: 1 - impend: 1 - 80-proof: 1 - tear-stopp: 1 - ten-thousand: 1 - mother-in-law: 1 - honky-tonkin: 1 - clackity-clack: 1 - one-night-stand: 1 - luckiest: 1\n",
      "\n",
      "Removed all words occuring 10 or less times\n",
      "Reduced vocab from 27579 to 4411 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/Line counts/rap_rock_rb_country.csv\n"
     ]
    }
   ],
   "source": [
    "tpl_quants = 2\n",
    "\n",
    "for use_length in range(21):\n",
    "    documents = preprocess(df['lyrics'], debug=True, limit=cutoff, stem_words=stem, use_rid=rid, \n",
    "                           line_quants=line_quants, token_quants=token_quants, tpl_quants=tpl_quants, use_length=use_length)\n",
    "    run(documents, df, genres, 'Line counts', quants, limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92162c5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Counting Lines\n",
      "Found quantiles: [ 32.  42.  49.  56.  64.  73.  84.  95. 110. 389.] for [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0] (range 4 - 389)\n",
      "Removing punctuation\n",
      "Normalizing\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     10\u001b[39m use_length = \u001b[32m16\u001b[39m\n\u001b[32m     12\u001b[39m df = pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mdata/song_lyrics_reduced_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m.join(genres)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlimit\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.csv\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m documents = \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlyrics\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstem_words\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_rid\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                           \u001b[49m\u001b[43mline_quants\u001b[49m\u001b[43m=\u001b[49m\u001b[43mline_quants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken_quants\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken_quants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtpl_quants\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtpl_quants\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_length\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m X_train, X_test, y_train, y_test = train_test_split(documents, df[\u001b[33m'\u001b[39m\u001b[33mtag\u001b[39m\u001b[33m'\u001b[39m], test_size=\u001b[32m0.1\u001b[39m, random_state = \u001b[32m42\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ngram \u001b[38;5;129;01min\u001b[39;00m [(\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m), (\u001b[32m2\u001b[39m, \u001b[32m2\u001b[39m), (\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m), (\u001b[32m3\u001b[39m, \u001b[32m3\u001b[39m), (\u001b[32m2\u001b[39m, \u001b[32m3\u001b[39m), (\u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m)]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\DataPrep.py:207\u001b[39m, in \u001b[36mpreprocess\u001b[39m\u001b[34m(docs, stem_words, limit, debug, return_count, use_rid, line_quants, token_quants, tpl_quants, use_length)\u001b[39m\n\u001b[32m    204\u001b[39m     line_counts = [len(document.split('\\n')) for document in noteless]\n\u001b[32m    205\u001b[39m     line_quantiles, quantiled_lines = get_quantiles(line_counts, line_quants, debug=debug)\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m # Punctuation\n\u001b[32m    208\u001b[39m if debug: print(\"Removing punctuation\")\n\u001b[32m    209\u001b[39m just_words = remove_punctuation(noteless)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\DataPrep.py:53\u001b[39m, in \u001b[36mget_rid_values\u001b[39m\u001b[34m(documents)\u001b[39m\n\u001b[32m     51\u001b[39m results = []\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m document \u001b[38;5;129;01min\u001b[39;00m documents:\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     result = \u001b[43mRID\u001b[49m\u001b[43m.\u001b[49m\u001b[43manalyze\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     counts = {category: \u001b[32m0\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m CATEGORIES}\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m result.category_count.items():\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\rid.py:129\u001b[39m, in \u001b[36mRegressiveImageryDictionary.analyze\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m tokens:\n\u001b[32m    128\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.token_is_excluded(token):\n\u001b[32m--> \u001b[39m\u001b[32m129\u001b[39m         category = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mget_category\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    130\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m category != \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    131\u001b[39m             increment_category(category, token)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\rid.py:111\u001b[39m, in \u001b[36mRegressiveImageryDictionary.get_category\u001b[39m\u001b[34m(self, word)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_category\u001b[39m(\u001b[38;5;28mself\u001b[39m, word):\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     categories = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpattern_tree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m categories:\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m categories[\u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\rid.py:395\u001b[39m, in \u001b[36mDiscriminationTree.retrieve\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    393\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    394\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m395\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnext_disc_tree\u001b[49m\u001b[43m.\u001b[49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\rid.py:391\u001b[39m, in \u001b[36mDiscriminationTree.retrieve\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    390\u001b[39m     next_index = path[\u001b[32m0\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m     next_disc_tree = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchild_matching_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    392\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m next_disc_tree == \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    393\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\rid.py:382\u001b[39m, in \u001b[36mDiscriminationTree.child_matching_index\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m    380\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mchild_matching_index\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m    381\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.interiors:\n\u001b[32m--> \u001b[39m\u001b[32m382\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m child.index == index:\n\u001b[32m    383\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m child\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# N-grams\n",
    "genres = ['rap', 'rock', 'rb', 'country']\n",
    "limit = 1000\n",
    "cutoff = 10\n",
    "stem = True\n",
    "rid=0\n",
    "line_quants = 10\n",
    "token_quants = 18\n",
    "tpl_quants = 2\n",
    "use_length = 16\n",
    "\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "documents = preprocess(df['lyrics'], debug=True, limit=cutoff, stem_words=stem, use_rid=rid,\n",
    "                           line_quants=line_quants, token_quants=token_quants, tpl_quants=tpl_quants, use_length=use_length)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, df['tag'], test_size=0.1, random_state = 42)\n",
    "\n",
    "for ngram in [(1, 1), (2, 2), (1, 2), (3, 3), (2, 3), (1, 3)]:\n",
    "    vectorizer, dt_matrix = vectorize(X_train, n_gram=ngram)\n",
    "    model = MultinomialNB()\n",
    "    model.fit(dt_matrix, y_train)\n",
    "\n",
    "    dt_matrix_test = vectorizer.transform(X_test)\n",
    "    y_pred = model.predict(dt_matrix_test)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    save_output(limit, genres, 'N-gram', f'{ngram[0]}-{ngram[1]}', accuracy, f1, additional=[('rid', rid)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2377c46a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpprojectvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
