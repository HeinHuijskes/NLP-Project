{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7be5ee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Hein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Hein\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from time import time\n",
    "\n",
    "from DataPrep import preprocess\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9db589db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(X_train):\n",
    "    vectorizer = CountVectorizer()\n",
    "    dt_matrix = vectorizer.fit_transform(X_train).toarray()\n",
    "    return vectorizer, dt_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8d99805",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_output(lim, genres, parameter, amount, accuracy, f1score, additional: list[tuple] = []):\n",
    "    '''Save the output of the tests to a csv file'''\n",
    "    # Create the location of the file\n",
    "    location = f'./output/{lim}/{parameter}/'\n",
    "    Path(location).mkdir(parents=True, exist_ok=True)\n",
    "    file = location + f'{\"_\".join(genres)}.csv'\n",
    "\n",
    "    # Prep csv format data\n",
    "    columns = [parameter, 'accuracy', 'f1score']\n",
    "    data = [[amount, accuracy, f1score]]\n",
    "    # Add optional additional parameters (like feature space size)\n",
    "    for param, amt in additional:\n",
    "        columns.append(param)\n",
    "        data[0].append(amt)\n",
    "\n",
    "    # Write to csv\n",
    "    df = pd.DataFrame(data, columns=columns)\n",
    "    header = not os.path.exists(file)\n",
    "    df.to_csv(file, mode='a', header=header)\n",
    "\n",
    "    print(f'Succesfully saved output to file {file}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40bf9018",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(documents, df, genres, parameter, amount, additional=[]):\n",
    "    '''Run the test and save the output'''\n",
    "    # Prepare data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(documents, df['tag'], test_size=0.1, random_state = 42)\n",
    "    vectorizer, dt_matrix = vectorize(X_train)\n",
    "\n",
    "    # Fit model\n",
    "    if parameter != 'model':\n",
    "        model = MultinomialNB()\n",
    "    else:\n",
    "        model = amount\n",
    "        amount = model.__name__\n",
    "    model.fit(dt_matrix, y_train)\n",
    "\n",
    "    # Test\n",
    "    dt_matrix_test = vectorizer.transform(X_test)\n",
    "    y_pred = model.predict(dt_matrix_test)\n",
    "\n",
    "    # Calculate and save results\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    save_output(limit, genres, parameter, amount, accuracy, f1, additional=additional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae158690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# genres = ['rap', 'rock', 'pop']\n",
    "# limit = 1000\n",
    "# df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "\n",
    "# # # Test stemming vs lemmatization\n",
    "# # for method in ['stem', 'lemmatize']:\n",
    "# #     documents = preprocess(df['lyrics'], stem_words=method=='stem', debug=False)\n",
    "# #     run(documents, df, genres, 'stem', method)\n",
    "\n",
    "# # Test effect of removing rare words\n",
    "# for cutoff in range(1, 50):\n",
    "#     old, total, documents = preprocess(df['lyrics'], limit=cutoff, debug=False, return_count=True)\n",
    "#     run(documents, df, genres, 'cutoff', cutoff, additional=[('features', total)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ddd22805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff-mid/rap_rock_pop.csv\n"
     ]
    }
   ],
   "source": [
    "genres = ['rap', 'rock', 'pop']\n",
    "limit = 1000\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "\n",
    "# Test effect of removing rare words\n",
    "gap = 20\n",
    "for cutoff in range(50):\n",
    "    old, total, documents = preprocess(df['lyrics'], limit=cutoff+gap, other_limit=cutoff, debug=False, return_count=True)\n",
    "    run(documents, df, genres, 'cutoff-mid', cutoff, additional=[('features', total)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7308a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully saved output to file ./output/1000/stem/rap_rock_pop_rb_country.csv\n",
      "Succesfully saved output to file ./output/1000/stem/rap_rock_pop_rb_country.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff/rap_rock_pop_rb_country.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff/rap_rock_pop_rb_country.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff/rap_rock_pop_rb_country.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff/rap_rock_pop_rb_country.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff/rap_rock_pop_rb_country.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff/rap_rock_pop_rb_country.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff/rap_rock_pop_rb_country.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff/rap_rock_pop_rb_country.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff/rap_rock_pop_rb_country.csv\n",
      "Succesfully saved output to file ./output/1000/cutoff/rap_rock_pop_rb_country.csv\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Test effect of removing rare words\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m cutoff \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m11\u001b[39m, \u001b[32m50\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     old, total, documents = \u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mlyrics\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcutoff\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdebug\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_count\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     run(documents, df, genres, \u001b[33m'\u001b[39m\u001b[33mcutoff\u001b[39m\u001b[33m'\u001b[39m, cutoff, additional=[(\u001b[33m'\u001b[39m\u001b[33mfeatures\u001b[39m\u001b[33m'\u001b[39m, total)])\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\DataPrep.py:149\u001b[39m, in \u001b[36mpreprocess\u001b[39m\u001b[34m(docs, stem_words, limit, debug, return_count)\u001b[39m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug: \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mNormalizing\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    148\u001b[39m normalized = normalize(just_words)\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug: \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTokenizing\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m tokens = tokenize(normalized)\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug: \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mRemoving stopwords\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\DataPrep.py:46\u001b[39m, in \u001b[36mtokenize\u001b[39m\u001b[34m(docs)\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m'''Tokenize documents using nltk'''\u001b[39;00m\n\u001b[32m     45\u001b[39m result = []\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m docs:\n\u001b[32m     47\u001b[39m     new_doc = word_tokenize(doc)\n\u001b[32m     48\u001b[39m     result.append(new_doc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\nlpprojectvenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:143\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m:type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\nlpprojectvenv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:144\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03mReturn a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03musing NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m:type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    142\u001b[39m sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m     token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[43m_treebank_word_tokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    145\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32me:\\OneDrive\\Study\\CS Master\\Natural Language Processing\\Project\\nlpprojectvenv\\Lib\\site-packages\\nltk\\tokenize\\destructive.py:161\u001b[39m, in \u001b[36mNLTKWordTokenizer.tokenize\u001b[39m\u001b[34m(self, text, convert_parentheses, return_str)\u001b[39m\n\u001b[32m    158\u001b[39m     text = regexp.sub(substitution, text)\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m regexp, substitution \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.PUNCTUATION:\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     text = regexp.sub(substitution, text)\n\u001b[32m    163\u001b[39m \u001b[38;5;66;03m# Handles parentheses.\u001b[39;00m\n\u001b[32m    164\u001b[39m regexp, substitution = \u001b[38;5;28mself\u001b[39m.PARENS_BRACKETS\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "genres = ['rap', 'rock', 'pop', 'rb', 'country']\n",
    "limit = 100\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "\n",
    "# Test stemming vs lemmatization\n",
    "for method in ['stem', 'lemmatize']:\n",
    "    documents = preprocess(df['lyrics'], stem_words=method=='stem', debug=False)\n",
    "    run(documents, df, genres, 'stem', method)\n",
    "\n",
    "# Test effect of removing rare words\n",
    "for cutoff in range(21, 50):\n",
    "    old, total, documents = preprocess(df['lyrics'], limit=cutoff, debug=False, return_count=True)\n",
    "    run(documents, df, genres, 'cutoff', cutoff, additional=[('features', total)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023dae77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully saved output to file ./output/1000/model/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/model/rap_rock_pop_rb_country.csv\n"
     ]
    }
   ],
   "source": [
    "genres = ['rap', 'rock', 'pop']\n",
    "limit = 100\n",
    "\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "# Prepare data\n",
    "documents = preprocess(df['lyrics'], debug=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, df['tag'], test_size=0.1, random_state = 42)\n",
    "vectorizer, dt_matrix = vectorize(X_train)\n",
    "\n",
    "# Fit model\n",
    "start = time()\n",
    "model = MultinomialNB()\n",
    "model.fit(dt_matrix, y_train)\n",
    "end = time() - start\n",
    "\n",
    "# Test\n",
    "dt_matrix_test = vectorizer.transform(X_test)\n",
    "y_pred = model.predict(dt_matrix_test)\n",
    "\n",
    "# Calculate and save results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "save_output(limit, genres, 'model', 'MultinomialNB', accuracy, f1, additional=[('time', end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ab3d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['rap', 'rock', 'pop', 'rb', 'country']\n",
    "limit = 100\n",
    "\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "# Prepare data\n",
    "documents = preprocess(df['lyrics'], debug=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, df['tag'], test_size=0.1, random_state = 42)\n",
    "vectorizer, dt_matrix = vectorize(X_train)\n",
    "\n",
    "# Fit model\n",
    "start = time()\n",
    "model = MultinomialNB()\n",
    "model.fit(dt_matrix, y_train)\n",
    "end = time() - start\n",
    "\n",
    "# Test\n",
    "dt_matrix_test = vectorizer.transform(X_test)\n",
    "y_pred = model.predict(dt_matrix_test)\n",
    "\n",
    "# Calculate and save results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "save_output(limit, genres, 'model', 'MultinomialNB', accuracy, f1, additional=[('time', end)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4701cb3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully saved output to file ./output/1000/model/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/model/rap_rock_pop_rb_country.csv\n"
     ]
    }
   ],
   "source": [
    "genres = ['rap', 'rock', 'pop']\n",
    "limit = 100\n",
    "\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "# Prepare data\n",
    "documents = preprocess(df['lyrics'], debug=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, df['tag'], test_size=0.1, random_state = 42)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.fit_transform(y_test)\n",
    "X_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Fit model\n",
    "start = time()\n",
    "model = SVC(C=1000000.0, kernel='linear', degree=3, gamma='auto')\n",
    "model.fit(X_train, y_train)\n",
    "end = time() - start\n",
    "\n",
    "# Test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and save results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "save_output(limit, genres, 'model', 'SVM', accuracy, f1, additional=[('time', end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffd4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['rap', 'rock', 'pop', 'rb', 'country']\n",
    "limit = 100\n",
    "\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "# Prepare data\n",
    "documents = preprocess(df['lyrics'], debug=False)\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, df['tag'], test_size=0.1, random_state = 42)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(y_train)\n",
    "y_test = label_encoder.fit_transform(y_test)\n",
    "X_train = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# Fit model\n",
    "start = time()\n",
    "model = SVC(C=1000000.0, kernel='linear', degree=3, gamma='auto')\n",
    "model.fit(X_train, y_train)\n",
    "end = time() - start\n",
    "\n",
    "# Test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate and save results\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "save_output(limit, genres, 'model', 'SVM', accuracy, f1, additional=[('time', end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be639cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing data\n",
      "Removing notes in [brackets]\n",
      "Removing punctuation\n",
      "Normalizing\n",
      "Tokenizing\n",
      "Removing stopwords\n",
      "Stemming\n",
      "Removing rare words\n",
      "Total words: 383130\n",
      "Unique words: 22033\n",
      "Most frequent:\n",
      "'s: 2610; n't: 2431; 'm: 2089; like: 2059; know: 1997; got: 1894; get: 1810; go: 1618; ': 1571; see: 1531\n",
      "Least frequent:\n",
      "heav'n: 1 - vauntingli: 1 - hirel: 1 - freemen: 1 - essex: 1 - zed: 1 - mortifi: 1 - villan: 1 - elaps: 1 - scroog: 1\n",
      "\n",
      "Removed all words occuring 25 or less times\n",
      "Reduced vocab from 22033 to 2223 words\n",
      "Finished data preparation!\n",
      "Succesfully saved output to file ./output/1000/n-gram-reduced-more/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/n-gram-reduced-more/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/n-gram-reduced-more/rap_rock_pop.csv\n",
      "Succesfully saved output to file ./output/1000/n-gram-reduced-more/rap_rock_pop.csv\n"
     ]
    }
   ],
   "source": [
    "# N-gram tests\n",
    "genres = ['rap', 'rock', 'pop']\n",
    "limit = 100\n",
    "\n",
    "# Prepare data\n",
    "df = pd.read_csv(f'data/song_lyrics_reduced_{\"_\".join(genres)}_{limit}.csv')\n",
    "documents = preprocess(df['lyrics'], limit=25)\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, df['tag'], test_size=0.1, random_state = 42)\n",
    "\n",
    "for n in range(1, 5):\n",
    "    # N_grams\n",
    "    n_vectorizer = CountVectorizer(analyzer='word', ngram_range=(n, n))\n",
    "    n_dt_matrix = n_vectorizer.fit_transform(X_train).toarray()\n",
    "\n",
    "    # Fit model\n",
    "    start = time()\n",
    "    model = MultinomialNB()\n",
    "    model.fit(n_dt_matrix, y_train)\n",
    "    end = time() - start\n",
    "\n",
    "    # Test\n",
    "    n_dt_matrix_test = n_vectorizer.transform(X_test)\n",
    "    y_pred = model.predict(n_dt_matrix_test)\n",
    "\n",
    "    # Calculate and save results\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "    save_output(limit, genres, 'n-gram-reduced-more', n, accuracy, f1, additional=[('time', end)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5231404e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlpprojectvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
